{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e3f1c8b",
   "metadata": {},
   "source": [
    "# Cryptocurrency Volatility Prediction\n",
    "\n",
    "**Single-file submission**: this notebook contains the full pipeline — data ingestion, preprocessing, feature engineering, EDA, model training, evaluation, artifact saving, and an optional Streamlit app. \n",
    "\n",
    "**Dataset source options:**\n",
    "- Use the uploaded `dataset.csv` in the same directory.\n",
    "- Or (reproducible) download from the provided Google Drive link.\n",
    "\n",
    "**How to use:** run the notebook top-to-bottom (Kernel ▶ Restart & Run All)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334efc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python -V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a8419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import pickle\n",
    "\n",
    "# Paths\n",
    "OUT_DIR = '/mnt/data/ml_submission_outputs'\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "DATA_CSV = 'dataset.csv'  # expected to be in the notebook working directory\n",
    "DRIVE_LINK = 'https://drive.google.com/file/d/1iVhJKnfAR-Vm4JHC-TY4-kXEcfH5C_ky/view?usp=drive_link'\n",
    "\n",
    "print('Output folder:', OUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eade1bfe",
   "metadata": {},
   "source": [
    "## Option A — Use uploaded `dataset.csv` (already provided)\n",
    "\n",
    "## Option B — Download dataset from Google Drive (reproducible)\n",
    "\n",
    "If you want the notebook to download the dataset automatically, uncomment and run the cell below. It uses `gdown` to fetch the file from Google Drive. If `gdown` is not installed, the cell will install it automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1bcce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to download dataset from Google Drive\n",
    "# !pip install --quiet gdown\n",
    "# import gdown\n",
    "# url = 'https://drive.google.com/uc?id=1iVhJKnfAR-Vm4JHC-TY4-kXEcfH5C_ky&export=download'\n",
    "# gdown.download(url, DATA_CSV, quiet=False)\n",
    "# print('Downloaded to', DATA_CSV)\n",
    "\n",
    "# If you already uploaded dataset.csv to the notebook folder, no need to run the downloader."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99de029",
   "metadata": {},
   "source": [
    "## Load dataset — robust to column name variations\n",
    "The dataset used in the PDF had columns like `crypto_name`, `marketCap`, `timestamp`, `date`, etc. This code will attempt to load and normalize those columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9006ba32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Try to read dataset\n",
    "if not os.path.exists(DATA_CSV):\n",
    "    raise FileNotFoundError(f\"{DATA_CSV} not found in working directory. Place the CSV next to this notebook or use the downloader cell.\")\n",
    "\n",
    "df = pd.read_csv(DATA_CSV)\n",
    "print('Initial shape:', df.shape)\n",
    "print('Columns:', df.columns.tolist())\n",
    "\n",
    "# Normalize column names\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "col_map = {}\n",
    "# common variants mapping\n",
    "variants = {\n",
    "    'crypto_name': 'symbol',\n",
    "    'name': 'symbol',\n",
    "    'marketCap': 'market_cap',\n",
    "    'market_cap': 'market_cap',\n",
    "    'timestamp': 'timestamp',\n",
    "    'date': 'date'\n",
    "}\n",
    "for c in df.columns:\n",
    "    lc = c.lower()\n",
    "    if lc in variants:\n",
    "        col_map[c] = variants[lc]\n",
    "    elif lc in ['open','high','low','close','volume']:\n",
    "        col_map[c] = lc\n",
    "    else:\n",
    "        # try fuzzy match\n",
    "        if 'name' in lc and 'crypto' in lc:\n",
    "            col_map[c] = 'symbol'\n",
    "        elif 'market' in lc and 'cap' in lc:\n",
    "            col_map[c] = 'market_cap'\n",
    "        elif 'time' in lc:\n",
    "            col_map[c] = 'timestamp'\n",
    "\n",
    "df = df.rename(columns=col_map)\n",
    "print('After rename - columns:', df.columns.tolist())\n",
    "\n",
    "# Ensure required columns exist\n",
    "required = ['date','symbol','open','high','low','close','volume','market_cap']\n",
    "missing = [c for c in required if c not in df.columns]\n",
    "if missing:\n",
    "    print('Warning - missing columns (these will be handled if present or approximated):', missing)\n",
    "\n",
    "# If date column exists but is not datetime, coerce it\n",
    "if 'date' in df.columns:\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "else:\n",
    "    # try to derive date from timestamp\n",
    "    if 'timestamp' in df.columns:\n",
    "        df['date'] = pd.to_datetime(df['timestamp'], errors='coerce').dt.date\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "    else:\n",
    "        raise ValueError('No date or timestamp column found. Cannot proceed without a date.')\n",
    "\n",
    "# Fill or coerce numeric cols\n",
    "for c in ['open','high','low','close','volume','market_cap']:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "\n",
    "# Quick look\n",
    "display(df.head())\n",
    "display(df.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a33f2e4",
   "metadata": {},
   "source": [
    "## Data cleaning, preprocessing, and feature engineering\n",
    "- Fill missing numeric values per symbol (forward/backfill)\n",
    "- Create returns, intraday_range, rolling stats, liquidity, ATR proxy\n",
    "- Define target `next_day_vol` = next day's intraday_range (you can change target later)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0a289b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Drop rows without date or symbol\n",
    "df = df.dropna(subset=['date']).copy()\n",
    "if 'symbol' not in df.columns:\n",
    "    # if no symbol, create one universal symbol\n",
    "    df['symbol'] = 'ALL'\n",
    "\n",
    "# Sort and fill numeric values per symbol\n",
    "df = df.sort_values(['symbol','date']).reset_index(drop=True)\n",
    "\n",
    "numeric_cols = [c for c in ['open','high','low','close','volume','market_cap'] if c in df.columns]\n",
    "if len(numeric_cols)==0:\n",
    "    raise ValueError('No numeric OHLC/volume/market_cap columns found. Please check your dataset.')\n",
    "\n",
    "# group-wise forward/backfill for numeric cols\n",
    "df[numeric_cols] = df.groupby('symbol')[numeric_cols].apply(lambda g: g.ffill().bfill())\n",
    "\n",
    "# Drop rows still missing core numeric data\n",
    "df = df.dropna(subset=['open','high','low','close']).reset_index(drop=True)\n",
    "\n",
    "# Feature engineering\n",
    "df['return'] = (df['close'] - df['open']) / df['open']\n",
    "df['intraday_range'] = (df['high'] - df['low']) / df['open']  # volatility proxy\n",
    "df['next_day_vol'] = df.groupby('symbol')['intraday_range'].shift(-1)\n",
    "\n",
    "# Rolling features\n",
    "df['ma_7'] = df.groupby('symbol')['close'].rolling(window=7, min_periods=1).mean().reset_index(level=0, drop=True)\n",
    "df['ma_30'] = df.groupby('symbol')['close'].rolling(window=30, min_periods=1).mean().reset_index(level=0, drop=True)\n",
    "df['vol_7'] = df.groupby('symbol')['return'].rolling(window=7, min_periods=1).std().reset_index(level=0, drop=True)\n",
    "df['vol_30'] = df.groupby('symbol')['return'].rolling(window=30, min_periods=1).std().reset_index(level=0, drop=True)\n",
    "df['liquidity'] = df['volume'] / (df['market_cap'].replace({0: np.nan}))\n",
    "df['atr_14'] = df['high'] - df['low']  # simplified ATR\n",
    "\n",
    "# Filter rows with target\n",
    "df_model = df.dropna(subset=['next_day_vol']).copy()\n",
    "df_model['symbol_code'] = df_model['symbol'].astype('category').cat.codes\n",
    "\n",
    "print('Final rows for modeling:', df_model.shape[0])\n",
    "display(df_model.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b6714e",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "Plots: target distribution, correlation heatmap, sample time-series for top symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9104a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Distribution of target\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.hist(df_model['next_day_vol'].dropna(), bins=80)\n",
    "plt.title('Distribution of Next-Day Volatility (intraday range proxy)')\n",
    "plt.xlabel('next_day_vol')\n",
    "plt.ylabel('count')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR,'eda_nextdayvol_dist.png'))\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmap (select numeric features)\n",
    "num_feats = ['open','high','low','close','volume','market_cap','return','intraday_range','ma_7','ma_30','vol_7','vol_30','liquidity']\n",
    "num_feats = [c for c in num_feats if c in df_model.columns]\n",
    "corr = df_model[num_feats].corr()\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(corr, annot=True, fmt='.2f', cmap='vlag')\n",
    "plt.title('Feature Correlation')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR,'eda_correlation.png'))\n",
    "plt.show()\n",
    "\n",
    "# Time series for top 3 symbols\n",
    "top_syms = df_model['symbol'].value_counts().nlargest(3).index.tolist()\n",
    "for sym in top_syms:\n",
    "    sub = df_model[df_model['symbol']==sym].sort_values('date')\n",
    "    plt.figure(figsize=(10,3))\n",
    "    plt.plot(sub['date'], sub['intraday_range'])\n",
    "    plt.title(f'Intraday Range over Time: {sym}')\n",
    "    plt.xlabel('date')\n",
    "    plt.ylabel('intraday_range')\n",
    "    plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.ConciseDateFormatter(mdates.AutoDateLocator()))\n",
    "    plt.tight_layout()\n",
    "    fname = os.path.join(OUT_DIR, f'ts_{sym}.png')\n",
    "    plt.savefig(fname)\n",
    "    plt.show()\n",
    "\n",
    "print('EDA plots saved to', OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd675b2f",
   "metadata": {},
   "source": [
    "## Model Training & Evaluation\n",
    "We'll train RandomForest and GradientBoosting and select the best by RMSE. The notebook uses a global time-based split (last 20% dates held out)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220050f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "feature_cols = ['open','high','low','close','volume','market_cap','return','intraday_range',\n",
    "                'ma_7','ma_30','vol_7','vol_30','liquidity','symbol_code']\n",
    "feature_cols = [c for c in feature_cols if c in df_model.columns]\n",
    "X = df_model[feature_cols].fillna(0)\n",
    "y = df_model['next_day_vol']\n",
    "\n",
    "# Time-based split by date (global)\n",
    "unique_dates = df_model['date'].sort_values().unique()\n",
    "split_date = unique_dates[int(len(unique_dates)*(1-0.2))] if len(unique_dates)>1 else unique_dates[0]\n",
    "train_mask = df_model['date'] <= split_date\n",
    "X_train, X_test = X[train_mask], X[~train_mask]\n",
    "y_train, y_test = y[train_mask], y[~train_mask]\n",
    "\n",
    "# Fallback to random split if needed\n",
    "if len(X_test)==0 or len(X_train)==0:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print('Train shape:', X_train.shape, 'Test shape:', X_test.shape)\n",
    "\n",
    "# Train models (small defaults)\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "gbr = GradientBoostingRegressor(n_estimators=200, learning_rate=0.05, random_state=42)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "gbr.fit(X_train, y_train)\n",
    "\n",
    "models = {'RandomForest': rf, 'GradientBoosting': gbr}\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    preds = model.predict(X_test)\n",
    "    rmse = mean_squared_error(y_test, preds, squared=False)\n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "    r2 = r2_score(y_test, preds)\n",
    "    results.append({'model': name, 'rmse': float(rmse), 'mae': float(mae), 'r2': float(r2)})\n",
    "results_df = pd.DataFrame(results).sort_values('rmse')\n",
    "display(results_df)\n",
    "\n",
    "best_model_name = results_df.iloc[0]['model']\n",
    "best_model = models[best_model_name]\n",
    "print('Best model:', best_model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b123d1aa",
   "metadata": {},
   "source": [
    "## Save artifacts\n",
    "Saves: cleaned dataset, feature dataset, model pickle, evaluation metrics, and EDA plots into `ml_submission_outputs/`. These files are suitable to include in a GitHub repo (or you can keep the notebook alone)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b7d1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save datasets and model\n",
    "cleaned_path = os.path.join(OUT_DIR, 'cleaned_dataset.csv')\n",
    "features_path = os.path.join(OUT_DIR, 'features_dataset.csv')\n",
    "model_path = os.path.join(OUT_DIR, 'best_model.pkl')\n",
    "metrics_path = os.path.join(OUT_DIR, 'evaluation_metrics.csv')\n",
    "\n",
    "df.to_csv(cleaned_path, index=False)\n",
    "df_model.to_csv(features_path, index=False)\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "\n",
    "results_df.to_csv(metrics_path, index=False)\n",
    "\n",
    "print('Saved:', cleaned_path)\n",
    "print('Saved:', features_path)\n",
    "print('Saved:', model_path)\n",
    "print('Saved:', metrics_path)\n",
    "print('Other plots saved to', OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dd4bdb",
   "metadata": {},
   "source": [
    "## Auto-generate README.md and requirements.txt for GitHub\n",
    "The next cell will write a `README.md` and `requirements.txt` tailored to this notebook. Run it to create those files in the output folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abc3f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "readme = f\"\"\"# Cryptocurrency Volatility Prediction\n",
    "\n",
    "This repository contains a single Jupyter Notebook that performs the full pipeline for predicting next-day cryptocurrency volatility using daily OHLC, volume, and market cap data.\n",
    "\n",
    "## Contents\n",
    "- `Crypto_Volatility_Prediction_Project.ipynb` — this notebook (primary)\n",
    "- `ml_submission_outputs/` — output folder with cleaned data, feature data, trained model, evaluation metrics and plots\n",
    "\n",
    "## How to run\n",
    "1. Place `dataset.csv` in the same folder as the notebook or use the Google Drive downloader cell.\n",
    "2. Open the notebook and run all cells (Kernel ▶ Restart & Run All).\n",
    "\n",
    "## Files produced (examples)\n",
    "- `ml_submission_outputs/cleaned_dataset.csv`\n",
    "- `ml_submission_outputs/features_dataset.csv`\n",
    "- `ml_submission_outputs/best_model.pkl`\n",
    "- `ml_submission_outputs/evaluation_metrics.csv`\n",
    "- `ml_submission_outputs/eda_nextdayvol_dist.png`\n",
    "\n",
    "## Notes\n",
    "- Target definition: next-day intraday range `(high-low)/open` used as a volatility proxy. You can modify the target to use a rolling standard deviation if preferred.\n",
    "- The notebook includes a Streamlit app snippet you can use for local deployment (see the bottom of the notebook).\n",
    "\n",
    "\"\"\"\n",
    "with open(os.path.join(OUT_DIR,'README.md'),'w') as f:\n",
    "    f.write(readme)\n",
    "\n",
    "reqs = [\n",
    "    'numpy', 'pandas', 'matplotlib', 'seaborn', 'scikit-learn', 'joblib', 'streamlit', 'gdown'\n",
    "]\n",
    "with open(os.path.join(OUT_DIR,'requirements.txt'),'w') as f:\n",
    "    f.write('\\n'.join(reqs))\n",
    "\n",
    "print('README.md and requirements.txt created in', OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b691870",
   "metadata": {},
   "source": [
    "## Streamlit app (optional)\n",
    "This cell writes a `app.py` Streamlit application that loads the pickled model and allows interactive input. Run it to create `app.py` in the output folder. To run locally: `streamlit run app.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17cfe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "app_code = \"\"\"import streamlit as st\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "st.title('Cryptocurrency Volatility Prediction')\n",
    "model_path = 'ml_submission_outputs/best_model.pkl'\n",
    "\n",
    "@st.cache_resource\n",
    "def load_model():\n",
    "    with open(model_path,'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "model = load_model()\n",
    "\n",
    "st.write('Model loaded from', model_path)\n",
    "\n",
    "st.markdown('Upload a CSV with the same feature columns used during training.')\n",
    "uploaded = st.file_uploader('Upload CSV', type=['csv'])\n",
    "if uploaded is not None:\n",
    "    df = pd.read_csv(uploaded)\n",
    "    st.write('Preview', df.head())\n",
    "    preds = model.predict(df)\n",
    "    st.write('Predictions (next-day volatility proxy):')\n",
    "    st.write(preds)\n",
    "\"\"\"\n",
    "\n",
    "with open(os.path.join(OUT_DIR,'app.py'),'w') as f:\n",
    "    f.write(app_code)\n",
    "print('Streamlit app written to', os.path.join(OUT_DIR,'app.py'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa42c62c",
   "metadata": {},
   "source": [
    "## Final notes\n",
    "- This notebook is ready to be uploaded to GitHub as the single-file submission. The `ml_submission_outputs/` folder will contain all derived files after running.\n",
    "- If you want, I can also zip the `ml_submission_outputs/` folder and provide it for download, or I can commit everything into a GitHub repo for you (if you provide access/token)."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
